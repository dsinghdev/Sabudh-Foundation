{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing A Multi-layered Perceptron Using Numpy"
      ],
      "metadata": {
        "id": "He-gNsJp7GDV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This coursework requires you to write your own implementation of the backpropagation algorithm for training your own neural network. You are required to do this assignment in the python  (python version 3) programming language, using only numpy and/or scipy.\n",
        "\n",
        "The goal of this assignment is to label images of 10 handwritten digits of “zero”, “one”,...,  “nine”. The images are 28 by 28 in size (mnist dataset), which will be represented as a vector x of  dimension 784 by listing all the pixel values in raster scan order. The labels t are 0,1,2,...,9 corresponding  to 10 classes as written in the image. There are 60000 training cases, containing 6000 examples of each of  10 classes. \n",
        "\n",
        "The way you choose to design your code for this homework will affect how much time you  spend coding. We recommend that you look through all of the problems before attempting the first  problem. A good foundation will make the rest of these problems easier. Use object oriented principles to code layers, activation function etc as classes. \n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "eh8P0Py_65I1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKh_SwPV2L2m",
        "outputId": "f596192e-808c-45c9-c2c6-131d293b3f75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd \n",
        "import os\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 1"
      ],
      "metadata": {
        "id": "LJiYmW-17b4R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here you must read an input file. Each line contains 785 numbers (comma delimited): the first number in each row denotes the class label: 0 corresponds to digit 0, 1 corresponds to digit 1, etc. The rest of the values are the 784 pixel values between 0 and 255 correspondig to black and white images.  As a warm up  question, load the data.\n",
        "\n",
        "For this problem you must write a function that takes a file path as an argument which contains  this data. Your function must return two values (x and y) that contains the data from the file as  described. Specifically, the first return value (x) must be a matrix where the rows are individual  examples of images, and the columns are individual pixels (n x 784 matrix). The second return value  must be a list/array of real numbers representing the labels of the examples (rows) in x. \n",
        "\n",
        "eg: \n",
        "\n",
        "1.0,0.0,1.0,0.0,....0.0,0.25,0.0,0.0 \n",
        "\n",
        "... \n",
        "\n",
        "1.0,0.0,1.0,0.0,...,1.0,0.0,0.0,0.96776 \n",
        "\n",
        "x = [ \n",
        "\n",
        "[1.0,0.0,1.0,0.0,....0.0,0.25,0.0,0.0] \n",
        "\n",
        "... \n",
        "\n",
        "[1.0,0.0,1.0,0.0,...,1.0,0.0,0.0,0.96776] \n",
        "\n",
        "]\n",
        "\n",
        "y = [5,...,2]"
      ],
      "metadata": {
        "id": "wTGFwW5E7hRh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class load_data:\n",
        "\n",
        "  def __init__(self,path):\n",
        "    self.path = path\n",
        "    self.read_file()\n",
        "\n",
        "  '''\n",
        "  Function for reading the data \n",
        "  '''\n",
        "  def read_file(self):\n",
        "  #loading the file using loadtxt function in numpy \n",
        "    data = np.loadtxt(self.path,delimiter=\",\",skiprows=1)\n",
        "    self.Y = data[:, 0].astype(int)          # <slice> = <array>[start_row:end_row, start_col:end_col]\n",
        "    self.X = data[:, 1:]\n",
        "    \n",
        "  '''\n",
        "  Funtion which return data outside class when called \n",
        "  '''\n",
        "  def get_data(self):\n",
        "    return self.X,self.Y"
      ],
      "metadata": {
        "id": "N3sz6n9ZsLt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/drive/MyDrive/data_set/mnist_train.csv\"      #path where the file is stored "
      ],
      "metadata": {
        "id": "jSv3WEWZ8v-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mnist = load_data(path)                    # making object "
      ],
      "metadata": {
        "id": "wUOaIqVLYJIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X,Y=mnist.get_data()         #return X and y "
      ],
      "metadata": {
        "id": "ZO87xNyQYTdY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X,X.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFXREktd_u1_",
        "outputId": "745c4ff3-4291-4001-9c0c-3e84588988e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]] (60000, 784)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(Y,Y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IISUnfHY_xK7",
        "outputId": "52df2704-7f5d-46d1-af41-002b002f5d8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5 0 4 ... 5 6 8] (60000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 2 "
      ],
      "metadata": {
        "id": "xia537rG7zky"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement the backpropagation algorithm in a zero hidden layer neural network (weights between input and output nodes). The output layer should be a softmax output over 10 classes corresponding to 10 classes of handwritten digits (e.g. An architecture: 784 > 10). Your backprop code should minimize the cross-entropy function for multi-class classification problems (categorical  cross entropy). \n",
        "\n",
        " \n",
        "\n",
        "where j is the class label\n",
        "\n",
        "This step should be done with a full step of gradient descent, not stochastic gradient descent or rmsprop. For this  problem you must write a function that takes as an input a matrix of x values, a list of y values (as  returned from problem 1), a weight matrix, and a learning rate and performs a single step of  backpropagation. You will need to do both a forward step with the inputs, and then a backward prop to  get the gradients. Return the updated weight matrix and bias in the same format as it was passed.\n",
        "\n",
        "The list of weight matrices will be a list with 1 entry where the only entry is a matrix in the  format where the rows represent all of the outgoing weights for a neuron in the input layer and the  columns represent the weights for the incoming neurons. A specific row column index will give you the  weight for a neuron to neuron connection. \n",
        "\n",
        "The list of bias vectors will be in the form where each entry in the list is a vector with the same  length as the first set of weights. (e.g. For an architecture of 784 > 10, there will be a single element list  with a vector of size 10)."
      ],
      "metadata": {
        "id": "dVjP3dyn72GV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(y_actual,y_pred):\n",
        "    '''\n",
        "    Calculating cost \n",
        "    '''\n",
        "    cost = -np.sum(y_actual*np.log(y_pred))       #calculating the cost      categorical cross entropy \n",
        "    return cost/y_actual.shape[0]"
      ],
      "metadata": {
        "id": "l15nFZ78VpAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #softmax function \n",
        "\n",
        "def softmax(x, derivative=False):\n",
        "    exp_x = np.exp(x)  \n",
        "    softmax = exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "    if derivative:\n",
        "        return softmax * (1 - softmax)\n",
        "    return softmax"
      ],
      "metadata": {
        "id": "UPVxgn9JZ9KL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot_encoded(x):\n",
        "    '''\n",
        "    one hot encoding \n",
        "    '''\n",
        "    n_values = 10     #number of categories (for mnist categories = 10)\n",
        "    y=np.eye(n_values)[x]     \n",
        "    return y"
      ],
      "metadata": {
        "id": "gL-I5cl83MbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def neural_network(X,Y,layer_nodes,epochs,learning_rate,momentum):\n",
        "  \n",
        "  weights = np.random.rand(layer_nodes[0],layer_nodes[len(layer_nodes)-1])*0.01  #  matrix containing weights between ouput and input layer \n",
        "  bias = np.random.rand(1,layer_nodes[len(layer_nodes)-1])*0.01\n",
        "  Vdw = np.zeros_like(weights) # initializing the momentum term\n",
        "  Vdb = np.zeros_like(bias) # initializing the momentum term\n",
        "  y = one_hot_encoded(Y)\n",
        "  \n",
        "  x = X/255.0\n",
        "\n",
        "  for i in range(epochs):\n",
        "    \n",
        "    #forward prop\n",
        "    Z  = np.add((np.matmul(x,weights)),bias)  \n",
        "    y_pred = A = softmax(Z)\n",
        "    \n",
        "    #cost calulation\n",
        "\n",
        "    cost =  loss(y,y_pred)\n",
        "\n",
        "    #back_prop\n",
        "    #updatweing weights \n",
        "    delta = (y_pred - y)\n",
        "    grad_w = np.matmul(x.T, delta)/y.shape[0]\n",
        "    Vdw = (momentum)*Vdw + (1- momentum)*grad_w                         \n",
        "    weights = weights -  learning_rate*Vdw\n",
        "\n",
        "    #updating bias \n",
        "    grad_B = np.sum(delta, axis=0, keepdims=True) / y.shape[0]\n",
        "    Vdb = (momentum)*Vdb + (1- momentum)*grad_B\n",
        "    bias = bias - learning_rate*Vdb\n",
        "\n",
        "    if i%10 == 0:\n",
        "      print(\"Epochs :\",i)\n",
        "      y_temp = np.argmax(y_pred, axis = 1)\n",
        "      accuracy = (Y == y_temp).sum() / len(Y)\n",
        "      print(f\"Accuracy: {accuracy*100} Cost:{cost}\")\n",
        "    elif i == epochs-1:\n",
        "      print('training completed')\n",
        "\n",
        "  return weights,bias "
      ],
      "metadata": {
        "id": "l9TazL3sVPeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weight,bias=neural_network(X,Y,[784,10],100,0.05,0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MnTEDdALY2-_",
        "outputId": "7927b1dd-2dbb-4e9f-95fa-082af9573d1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs : 0\n",
            "Accuracy: 10.783333333333333 Cost:2.305870774569131\n",
            "Epochs : 10\n",
            "Accuracy: 74.88166666666667 Cost:1.8577564722962943\n",
            "Epochs : 20\n",
            "Accuracy: 77.71666666666667 Cost:1.5518255795233806\n",
            "Epochs : 30\n",
            "Accuracy: 79.46 Cost:1.3405131577699605\n",
            "Epochs : 40\n",
            "Accuracy: 80.68833333333333 Cost:1.1906338986975964\n",
            "Epochs : 50\n",
            "Accuracy: 81.62166666666667 Cost:1.0804939637095197\n",
            "Epochs : 60\n",
            "Accuracy: 82.31333333333333 Cost:0.9966955091681919\n",
            "Epochs : 70\n",
            "Accuracy: 82.88333333333333 Cost:0.9309556117369875\n",
            "Epochs : 80\n",
            "Accuracy: 83.32166666666667 Cost:0.8780235337240258\n",
            "Epochs : 90\n",
            "Accuracy: 83.74166666666667 Cost:0.8344619646845146\n",
            "training completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weight.shape,weight"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7d9lYozAGH_M",
        "outputId": "e03fdedd-5c7b-4a0e-af6a-735424a6c48a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((784, 10),\n",
              " array([[0.0037454 , 0.00950714, 0.00731994, ..., 0.00866176, 0.00601115,\n",
              "         0.00708073],\n",
              "        [0.00020584, 0.0096991 , 0.00832443, ..., 0.00524756, 0.00431945,\n",
              "         0.00291229],\n",
              "        [0.00611853, 0.00139494, 0.00292145, ..., 0.00514234, 0.00592415,\n",
              "         0.0004645 ],\n",
              "        ...,\n",
              "        [0.0022309 , 0.00056516, 0.00103395, ..., 0.00940537, 0.00905944,\n",
              "         0.00566806],\n",
              "        [0.00354424, 0.00381147, 0.00411852, ..., 0.00335946, 0.00560694,\n",
              "         0.00095057],\n",
              "        [0.00200469, 0.00413466, 0.00206203, ..., 0.00691864, 0.00383069,\n",
              "         0.00869099]]))"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bias.shape,bias "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEsL1xecGNkC",
        "outputId": "98475a7e-5378-4a9c-e96d-8eb9db3d58b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1, 10),\n",
              " array([[-0.03061242,  0.08054823, -0.01777461, -0.01511964,  0.02659754,\n",
              "          0.03242076, -0.00094546,  0.03535134, -0.06060231, -0.00553034]]))"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 3 "
      ],
      "metadata": {
        "id": "RP9Gvtq17_NP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extend your code from problem 2 to support a single layer neural network with n hidden units (e.g. An  architecture: 784 > 10 > 10). These hidden units should be using sigmoid activations. \n",
        "\n",
        "For this problem you must write a function that takes as an input a matrix of x values, a list of y  values (as returned from problem 1), list of weight matrices, a list of bias vectors, and a learning rate and performs a single step of backpropagation. You will need to do both a forward step with the inputs to get the outputs, and then a backward prop to get the gradients. Return the  updated weight matrix and bias in the same format as it was passed.\n",
        "\n",
        "The list of weight matrices is a list with 2 entries where each entry in the list contains a single weight matrix as previously defined in problem 2. For a network with shape 784 > 10 > 10 the passed list  of weight matrices would look like this: [matrix with shape 784x10, matrix with shape 10x10]. Note:  though a hidden layer of size 10 is used as an example here, your code must be able to support a hidden  layer of dimension n.\n",
        "\n",
        "The list of bias vectors will be in the form where each entry in the list is a vector with the same  length as the first set of weights. (e.g. For an architecture of 784 > 10 > 10, there will be a two element  list with an vector of size 10 and a vector of size 10) "
      ],
      "metadata": {
        "id": "rfttI1ok8EHj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#relu function \n",
        "def relu(x, derivative=False):\n",
        "    if derivative:\n",
        "        return np.where(x <= 0, 0, 1)\n",
        "    else:\n",
        "        return np.maximum(0, x)"
      ],
      "metadata": {
        "id": "Glw6O8cRxe6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tanh function \n",
        "def tanh(x,derivative=False):\n",
        "    t=(np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x)) \n",
        "    if derivative:\n",
        "         return (1-t**2)\n",
        "    return t"
      ],
      "metadata": {
        "id": "_nTCTKmvxSwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x,derivative =False):\n",
        "    sig = 1 / (1 + np.exp(-x))\n",
        "    if derivative:\n",
        "        return sig * (1 - sig)\n",
        "    return sig"
      ],
      "metadata": {
        "id": "ZXTOLgcp8OB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Activation(input,function_type,derivative=False):\n",
        "\n",
        "  if function_type == \"sigmoid\":\n",
        "    if derivative:\n",
        "      sigmoid(input,True)\n",
        "    return sigmoid(input,False)\n",
        "\n",
        "  if function_type == \"relu\":\n",
        "    if derivative:\n",
        "      relu(input,True)\n",
        "    return relu(input,False)\n",
        "\n",
        "  if function_type == \"tanh\":\n",
        "    if derivative:\n",
        "      tanh(input,True)\n",
        "    return tanh(input,False)\n"
      ],
      "metadata": {
        "id": "g_F0_65rjNJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking the data values \n",
        "\n",
        "lst = [None]*10\n",
        "for i in range(len(lst)):\n",
        "   count = 0\n",
        "   for j in Y:\n",
        "     if i == j:\n",
        "       count+=1\n",
        "       lst[i] = count\n",
        "\n",
        "print(lst)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1fhjyc0-qbR",
        "outputId": "cec6aa08-88ad-4117-81cd-05b0a85a3831"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5923, 6742, 5958, 6131, 5842, 5421, 5918, 6265, 5851, 5949]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def neural_network(X,Y,layer_nodes,epochs,activation_type,learning_rate,momentum,initialization):\n",
        "  weights = [None]*(len(layer_nodes)-1)\n",
        "  bias = [None]*(len(layer_nodes)-1)\n",
        "  Z = [None]*(len(layer_nodes)-1)\n",
        "  A = [None]*(len(layer_nodes)-1)\n",
        "  Vdw = [None]*(len(layer_nodes)-1)\n",
        "  Vdb = [None]*(len(layer_nodes)-1)\n",
        "  \n",
        "  \n",
        "  for i in range(len(layer_nodes)-1):\n",
        "     if initialization == \"xavier\":\n",
        "            # Xavier initialization \n",
        "            weights[i] = np.random.randn(layer_nodes[i], layer_nodes[i+1]) * np.sqrt(1/layer_nodes[i])\n",
        "     elif initialization == \"he\":\n",
        "            # He initialization \n",
        "            weights[i] = np.random.randn(layer_nodes[i], layer_nodes[i+1]) * np.sqrt(2/layer_nodes[i])\n",
        "     bias[i] = np.random.rand(1,layer_nodes[i+1])*0.01\n",
        "     Vdw[i] = np.zeros_like(weights[i]) # initializing the momentum term\n",
        "     Vdb[i] = np.zeros_like(bias[i]) # initializing the momentum term\n",
        "\n",
        "  y = one_hot_encoded(Y)\n",
        "  # training \n",
        "  x = X/255.0\n",
        "  for i in range(epochs):\n",
        "      \n",
        "    \n",
        "    inputs = x\n",
        "    #forward prop\n",
        "    for j in range(len(layer_nodes)-2):\n",
        "       Z[j]  = np.add(np.matmul(inputs,weights[j]),bias[j])\n",
        "       A[j] = Activation(Z[j],activation_type)\n",
        "       inputs =  A[j]\n",
        "      #  print(\"ssa\",Z[j][:2],A[j][:2])\n",
        "    Z[-1] = np.add(np.matmul(inputs,weights[-1]),bias[-1])\n",
        "    y_pred = A[-1] = softmax(Z[-1])\n",
        "    \n",
        "    \n",
        "    #cost calulation\n",
        "    cost =  loss(y,y_pred)\n",
        "  \n",
        "    #back_prop\n",
        "    \n",
        "    delta = [None] * (len(layer_nodes) - 1)\n",
        "    delta[-1] = y_pred - y\n",
        "    # print(\"delta -1\",delta[-1])\n",
        "    for j in range(len(layer_nodes) - 2, 0, -1):\n",
        "            delta[j-1] = (np.matmul(delta[j], weights[j].T) * Activation(Z[j-1], activation_type, True))\n",
        "\n",
        "        # weight and bias updates\n",
        "    for j in reversed(range(len(layer_nodes)-1)):\n",
        "\n",
        "            dW = np.matmul(A[j-1].T, delta[j])/y.shape[0]  if j != 0 else np.matmul(x.T, delta[j])/y.shape[0] \n",
        "            db = np.sum(delta[j], axis=0, keepdims=True)/y.shape[0] \n",
        "\n",
        "            Vdw[j] = momentum*Vdw[j] + (1-momentum)*dW\n",
        "            Vdb[j] = momentum*Vdb[j] + (1-momentum)*db\n",
        "\n",
        "            weights[j] = weights[j] - learning_rate * Vdw[j]\n",
        "            bias[j] = bias[j] - learning_rate * Vdb[j]\n",
        "\n",
        "    if i%10 == 0:\n",
        "      print(\"Epochs :\",i)\n",
        "      y_temp = np.argmax(y_pred, axis = 1)\n",
        "      accuracy = (Y == y_temp).sum() / len(Y)\n",
        "      print(f\"Accuracy: {accuracy*100} Cost:{cost}\")\n",
        "    elif i == epochs-1:\n",
        "      print('training completed')\n",
        "\n",
        "  return weights,bias "
      ],
      "metadata": {
        "id": "5x1AV7QgYb0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weight_hidden_1,bias_hidden_1 = neural_network(X,Y,[784,10,10],100,\"sigmoid\",0.05,0,\"xavier\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BYhK_zQcToE",
        "outputId": "6b4a11a6-81c6-4e29-8c95-301ead9480a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs : 0\n",
            "Accuracy: 9.871666666666666 Cost:2.501220155359159\n",
            "Epochs : 10\n",
            "Accuracy: 9.871666666666666 Cost:2.333733816528802\n",
            "Epochs : 20\n",
            "Accuracy: 10.523333333333333 Cost:2.2626854951222986\n",
            "Epochs : 30\n",
            "Accuracy: 13.113333333333333 Cost:2.2098079089282057\n",
            "Epochs : 40\n",
            "Accuracy: 16.53333333333333 Cost:2.1653831166839224\n",
            "Epochs : 50\n",
            "Accuracy: 22.925 Cost:2.129570187872095\n",
            "Epochs : 60\n",
            "Accuracy: 29.031666666666666 Cost:2.102033487702629\n",
            "Epochs : 70\n",
            "Accuracy: 31.838333333333335 Cost:2.0809716131614295\n",
            "Epochs : 80\n",
            "Accuracy: 33.68333333333333 Cost:2.064429273310835\n",
            "Epochs : 90\n",
            "Accuracy: 35.22833333333333 Cost:2.051235606757686\n",
            "training completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in weight_hidden_1:\n",
        " print(\"shape\",i.shape,i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0NIc1kGcvuk",
        "outputId": "dbe6f50d-fdad-412f-8de4-66f0dd139c18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape (784, 10) [[ 0.03867857 -0.03844219 -0.02406393 ... -0.01701978 -0.01067215\n",
            "  -0.01279444]\n",
            " [-0.01972362 -0.0391837   0.08185397 ...  0.00461384 -0.00225978\n",
            "  -0.02356245]\n",
            " [ 0.03513758  0.00449595 -0.01701969 ... -0.04273806 -0.0094843\n",
            "   0.04893221]\n",
            " ...\n",
            " [-0.00800861 -0.00752981  0.07983669 ... -0.04757592 -0.0289131\n",
            "   0.06792547]\n",
            " [-0.01864408  0.06854576 -0.0444058  ...  0.01609894 -0.0148684\n",
            "  -0.00476128]\n",
            " [-0.0113951  -0.0326662  -0.03171296 ... -0.02758959  0.02641997\n",
            "   0.05304345]]\n",
            "shape (10, 10) [[ 0.26076448 -0.08006469  0.31616795  0.31060359 -0.24993848 -0.20563951\n",
            "   0.43160543 -0.22737842 -0.00585578 -0.34983297]\n",
            " [-0.33010044  0.05204221 -0.29499973 -0.22014834  0.18863332 -0.30901265\n",
            "  -0.30771959 -0.15798602 -0.05709704  0.3151051 ]\n",
            " [ 0.38593846 -0.19185248  0.69187401 -0.28337538  0.39048068 -0.24849363\n",
            "  -0.30336313 -0.17322111 -0.06199649  0.17345506]\n",
            " [ 0.07355608 -0.30922649 -0.34820993 -0.17062544  0.02584715 -0.08015863\n",
            "  -0.38784719  0.21931706  0.6974042  -0.17056859]\n",
            " [-0.3786331   0.27242306 -0.17520595  0.40124841 -0.1926099   0.55673696\n",
            "   0.05916467 -0.11346218 -0.22175493 -0.21671859]\n",
            " [ 0.35874666 -0.45348865 -0.24232338 -0.02705418  0.01886856 -0.06802161\n",
            "  -0.53327806 -0.02620217 -0.06774834 -0.23424467]\n",
            " [ 0.09125034  0.39291734 -0.03746986  0.13980565 -0.67066393 -0.16002958\n",
            "   0.29057273 -0.04078977 -0.28737848  0.69224679]\n",
            " [ 0.553972    0.1569615   0.37423231 -0.6764684  -0.22418593 -0.16532833\n",
            "  -0.06791327 -0.07690182 -0.87831329  0.07176657]\n",
            " [ 0.06059782  0.62493383 -0.25298979 -0.2158043  -0.84252692 -0.10027052\n",
            "   0.27729869 -0.16647058  0.66048929 -0.1433757 ]\n",
            " [-0.13788943 -0.4828482   0.10779591 -0.58685375  0.3581506   0.17699988\n",
            "   0.47115267 -0.23224775  0.01005902  0.18084698]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in bias_hidden_1:\n",
        " print(\"shape\",i.shape,i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6XHGWADc1cD",
        "outputId": "0ac8d379-33d4-4d48-817e-45cac221524b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape (1, 10) [[ 0.03689095  0.05795016 -0.02511896  0.0157375   0.12489849  0.02142296\n",
            "  -0.00228715  0.02151841  0.02528147  0.03963077]]\n",
            "shape (1, 10) [[-0.32478797  0.10274106 -0.00376753  0.21034772  0.11782068 -0.05593662\n",
            "  -0.06920574  0.20041549 -0.07946515 -0.05747278]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 4 "
      ],
      "metadata": {
        "id": "eIXlHVhP8FyQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extend your code from problem 3 (use cross entropy error) and implement a multi-layer neural  network, starting with a simple architecture containing any number of hidden units in each layer (e.g. With  architecture: 784 > 10 > 10 > 10). These hidden units should be using sigmoid activations. \n",
        "\n",
        "For this problem you must write a function that takes as an input a matrix of x values, a list of y  values (as returned from problem 1), list of weight matrices, a list of bias vectors, and a learning rate and  performs a single step of backpropagation. You will need to do both a forward step with the inputs to  get the outputs, and then a backward prop to get the gradients. Return the updated weight matrix and  bias in the same format as it was passed. \n",
        "\n",
        "The list of weight matrices is a list with k entries where each entry in the list contains a single  weight matrix as previously defined in problem 2. For a network with shape 784 > 10 > 10 > 10 the  passed list of weight matrices would look like this: [matrix with shape 784x10, matrix with shape 10x10,  matrix with shape 10x10]. Note: though a hidden layer of size 10 is used as an example here, your code  must be able to support a hidden layer of dimension n. \n",
        "\n",
        "The list of bias vectors will be in the form where each entry in the list is a vector with the same  length as the first set of weights. (e.g. For an architecture of 784 > 10 > 10, there will be a two element  list with an vector of size 10 and a vector of size 10) "
      ],
      "metadata": {
        "id": "LUJF6Gm28ILT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weight_hidden_2_sigmoid,bias_hidden_2_sigmoid = neural_network(X,Y,[784,10,10,10],100,\"sigmoid\",0.25,0,\"he\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKVKN_PT8MlV",
        "outputId": "41eff1dc-b366-42fa-d257-56674307af7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs : 0\n",
            "Accuracy: 10.503333333333334 Cost:2.8010808720754703\n",
            "Epochs : 10\n",
            "Accuracy: 19.525000000000002 Cost:2.2967122016264527\n",
            "Epochs : 20\n",
            "Accuracy: 26.185000000000002 Cost:2.2145720657945023\n",
            "Epochs : 30\n",
            "Accuracy: 28.431666666666665 Cost:2.187854502871245\n",
            "Epochs : 40\n",
            "Accuracy: 27.058333333333334 Cost:2.1904211559567113\n",
            "Epochs : 50\n",
            "Accuracy: 28.396666666666665 Cost:2.1919251938225917\n",
            "Epochs : 60\n",
            "Accuracy: 27.99 Cost:2.1910532053721847\n",
            "Epochs : 70\n",
            "Accuracy: 27.66 Cost:2.189410100014983\n",
            "Epochs : 80\n",
            "Accuracy: 27.016666666666666 Cost:2.187746309595193\n",
            "Epochs : 90\n",
            "Accuracy: 26.218333333333334 Cost:2.1860472358553187\n",
            "training completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in weight_hidden_2_sigmoid:\n",
        "  print(\"shape\",i.shape,i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkTBq-v_Sqis",
        "outputId": "45b7063c-4ece-4708-b5ab-1e0c595c6be3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape (784, 10) [[ 0.06450809 -0.00300931 -0.01067949 ... -0.02219036 -0.08800531\n",
            "   0.01185121]\n",
            " [ 0.02460519 -0.00313512 -0.06421988 ... -0.05588706 -0.08918705\n",
            "  -0.00385846]\n",
            " [-0.05941697 -0.09461307 -0.01200104 ... -0.05864206  0.05051496\n",
            "  -0.0177138 ]\n",
            " ...\n",
            " [ 0.02418423  0.01104036 -0.05067011 ... -0.02472395 -0.01266672\n",
            "  -0.00710641]\n",
            " [-0.06233428  0.00131802 -0.03048509 ... -0.00871182  0.0316206\n",
            "   0.05538582]\n",
            " [ 0.01901513 -0.00961751 -0.05101269 ...  0.02330854  0.01144925\n",
            "  -0.03684521]]\n",
            "shape (10, 10) [[ 6.15387878e-02  6.42474682e-01 -4.60129764e-01  1.07805939e+00\n",
            "  -3.92333841e-01 -2.52568915e-01 -1.89582184e-01 -2.94343325e-01\n",
            "  -2.45857431e-02 -1.30054656e-01]\n",
            " [ 8.73335484e-01  8.60864329e-01 -6.48558692e-02  1.00825068e+00\n",
            "  -5.80607227e-01  1.20999601e-01 -5.30162268e-01 -2.23088902e-01\n",
            "  -4.95202576e-01 -6.69527461e-01]\n",
            " [-4.50494810e-04 -1.10452777e+00  2.99513265e-01 -4.37619788e-02\n",
            "   3.76978540e-01  6.97673637e-01 -1.52853353e+00  9.02103686e-01\n",
            "   9.65669075e-01 -4.25542222e-01]\n",
            " [ 2.03640224e+00  1.28938236e+00  5.25105396e-01  5.13428156e-02\n",
            "   5.17223933e-01  3.65400618e-01  8.08898176e-01  1.20324895e+00\n",
            "   1.34335209e-01 -1.71826701e-01]\n",
            " [-8.45355428e-01 -7.67548400e-01  4.65886328e-01  1.48368126e+00\n",
            "   3.04060013e-01  1.04709420e+00 -2.00396825e-01 -1.09896057e-01\n",
            "  -4.04379044e-01  4.29551606e-01]\n",
            " [-1.31318812e-01 -3.75736454e-01 -7.73355458e-01  9.90665065e-02\n",
            "  -9.52966306e-02  5.87165421e-02  1.17997030e+00 -7.68424680e-02\n",
            "   3.95110663e-02  3.16382935e-01]\n",
            " [-8.56700604e-01 -8.09616517e-01 -1.67419310e-01  3.58909266e-01\n",
            "  -1.87454587e-01  1.35736234e+00  5.59109886e-01 -3.57419706e-02\n",
            "  -2.43741991e-01  5.58823816e-01]\n",
            " [ 5.24926992e-01  9.23486206e-03 -7.09754928e-01  1.00408037e-01\n",
            "   3.41310681e-01 -8.11192613e-01  7.22285026e-01 -2.28966186e-01\n",
            "   4.95827814e-01  2.53133401e-01]\n",
            " [ 6.30676326e-02  1.94994870e-01  1.68228717e-01  9.73752764e-01\n",
            "   2.05764164e-02  9.79374720e-02  1.36098324e+00 -2.73593134e-01\n",
            "   6.49413208e-01  4.43583568e-01]\n",
            " [ 3.04486222e-01  1.28893046e+00 -4.89530366e-01  1.93327916e-01\n",
            "   5.73963446e-01 -3.79767926e-01  6.23299279e-01 -1.20651260e-01\n",
            "   4.39968706e-01 -2.63074042e-01]]\n",
            "shape (10, 10) [[-9.29261133e-03 -3.98882658e-05  6.06821366e-02 -9.49700458e-01\n",
            "   3.38341390e-01 -6.20910450e-01  5.01493752e-01  1.11790749e+00\n",
            "   6.51006449e-02  1.02347217e+00]\n",
            " [-5.54193888e-01 -6.69557089e-01 -2.63540731e-01 -2.74076454e-01\n",
            "  -7.44997621e-02 -4.01074052e-01  8.42555430e-01  2.68740033e-01\n",
            "  -2.91789382e-01  2.23585209e-01]\n",
            " [-1.77162841e-01  2.03527861e-01  5.20861535e-02 -7.04279831e-01\n",
            "   2.67194823e-01  4.77056213e-01  2.56817259e-01  2.61403151e-01\n",
            "  -1.39185920e-01  5.17516957e-02]\n",
            " [ 4.04258676e-01  6.06423259e-02 -8.44963157e-02  1.10618354e-01\n",
            "  -9.34523558e-01 -6.80140405e-01  2.66987799e-01 -2.34395949e-01\n",
            "   5.91646195e-03  2.95090968e-01]\n",
            " [ 1.85733467e-01 -1.03033637e-01 -2.09561083e-02  1.30400375e-01\n",
            "   2.95366976e-01 -2.21414210e-01 -6.79726818e-01 -9.75692079e-02\n",
            "   1.89358015e-01  1.57526101e-01]\n",
            " [-3.19424159e-01  9.79544246e-01 -7.46160227e-01  4.81974705e-02\n",
            "  -3.14584877e-01  5.57992840e-01 -5.40424550e-01 -3.29756654e-01\n",
            "   3.45279843e-01 -8.84179425e-01]\n",
            " [ 2.67733966e-01 -1.88740204e-01  8.31121943e-01 -2.06247226e-01\n",
            "   3.75098555e-02  5.24007253e-01  7.85237468e-01 -9.23560063e-01\n",
            "  -3.90820161e-01  2.00836979e-02]\n",
            " [-5.79182460e-01  3.63990123e-01 -3.21409638e-01 -4.39728209e-01\n",
            "   5.91301266e-01  3.91367445e-01 -1.11084812e+00  6.57283366e-01\n",
            "  -1.26356789e-01 -1.94745922e-01]\n",
            " [-8.06487089e-02 -4.16777599e-01 -5.55278169e-01  1.91851872e-01\n",
            "  -1.49689948e-01 -2.37915203e-01 -1.03388849e+00 -1.82085970e-01\n",
            "  -3.72933031e-01 -5.13351504e-01]\n",
            " [-6.33297082e-02  2.09740595e-02  6.27073555e-01  3.95647224e-01\n",
            "   7.53870698e-01  8.13678310e-01  2.27055854e-01 -8.18859551e-01\n",
            "   2.80213885e-02 -2.67923057e-01]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in bias_hidden_2_sigmoid:\n",
        "  print(\"shape\",i.shape,i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQnkGCPaSo3_",
        "outputId": "cc401718-4c73-428f-beba-bc390ecbf892"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape (1, 10) [[0.21577613 0.59704772 2.0038554  1.99248291 1.30604777 0.53564834\n",
            "  1.40188012 0.52966428 0.81184694 0.70863123]]\n",
            "shape (1, 10) [[ 0.12031284  0.3117412  -0.22507724  0.04197843  0.00116065  0.28282829\n",
            "   0.09068033 -0.05572946  0.1958926   0.00733357]]\n",
            "shape (1, 10) [[ 0.12688225 -0.2474263   0.14795507  0.68343244 -0.28381852 -0.27390201\n",
            "  -0.10141667  0.11308086  0.20373667 -0.33165242]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 5 "
      ],
      "metadata": {
        "id": "C2g1eIrf8QHa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extend your code from problem 4 to implement different activations functions which will be  passed as a parameter. In this problem all activations (except the final layer which should remain a  softmax) must be changed to the passed activation function. "
      ],
      "metadata": {
        "id": "tSky9-cL8SZx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tanh Activation Function "
      ],
      "metadata": {
        "id": "ioYVBcU6URbu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weight_hidden_2_tanh,bias_hidden_2_tanh = neural_network(X,Y,[784,10,10,10],100,\"tanh\",0.2,0,\"xavier\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VP8uHr_o8WRj",
        "outputId": "ee6498ee-da10-4e3b-a27d-99b33d9bf898"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs : 0\n",
            "Accuracy: 7.091666666666667 Cost:2.563581695518555\n",
            "Epochs : 10\n",
            "Accuracy: 7.363333333333333 Cost:2.447037352199792\n",
            "Epochs : 20\n",
            "Accuracy: 9.921666666666667 Cost:2.4222354513541147\n",
            "Epochs : 30\n",
            "Accuracy: 13.985 Cost:2.3217714891228014\n",
            "Epochs : 40\n",
            "Accuracy: 11.683333333333334 Cost:2.2805871027887723\n",
            "Epochs : 50\n",
            "Accuracy: 15.939999999999998 Cost:2.2529892305473296\n",
            "Epochs : 60\n",
            "Accuracy: 21.906666666666666 Cost:2.2059732793706788\n",
            "Epochs : 70\n",
            "Accuracy: 14.655000000000001 Cost:2.177787434209266\n",
            "Epochs : 80\n",
            "Accuracy: 15.486666666666668 Cost:2.229000105150733\n",
            "Epochs : 90\n",
            "Accuracy: 20.235 Cost:2.1780416784674084\n",
            "training completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in weight_hidden_2_tanh:\n",
        "  print(\"shape\",i.shape,i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFjYcRlVTa_w",
        "outputId": "268f4886-8281-4044-cbab-d2cd59b15449"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape (784, 10) [[-0.0316801  -0.02221189 -0.02684236 ...  0.01356259  0.09089857\n",
            "   0.06792709]\n",
            " [ 0.02977478  0.0382282  -0.00670879 ...  0.06046427 -0.0311955\n",
            "  -0.00999775]\n",
            " [ 0.00795794 -0.04667469 -0.17241928 ... -0.05858568 -0.02140373\n",
            "   0.05016318]\n",
            " ...\n",
            " [-0.08449199 -0.03421773  0.15887473 ...  0.09140153 -0.02354644\n",
            "   0.0189133 ]\n",
            " [ 0.01714805 -0.00845479  0.06478213 ... -0.09518251  0.03037768\n",
            "   0.02960935]\n",
            " [ 0.05711142  0.0394226  -0.01606055 ... -0.02964405  0.01599197\n",
            "  -0.01757267]]\n",
            "shape (10, 10) [[-9.00034923e-02 -5.51387281e-01 -1.03142279e-01  7.23509164e-01\n",
            "  -1.86963351e-01 -8.61467463e-01 -3.41908401e-01  9.36307606e-01\n",
            "  -5.66905682e-01 -7.69176064e-01]\n",
            " [ 2.65600477e-01 -3.30920447e-01 -7.61746753e-01  3.03596043e-01\n",
            "   1.81984788e-01 -1.62186006e-01 -7.91926646e-01  5.97094261e-01\n",
            "  -3.25800095e-01  7.75046651e-01]\n",
            " [ 2.12190548e-01  2.93558205e-01  3.20590963e-01 -2.08881746e-01\n",
            "   1.55131806e-01 -2.69819379e-01 -1.51594865e-02  1.15995342e+00\n",
            "   1.41839842e-01 -4.70997836e-02]\n",
            " [-7.73997963e-01 -2.48892707e-01  6.80724682e-02  3.31807830e-01\n",
            "   3.79711618e-01 -1.19756686e+00  3.54625276e-02  4.58333970e-01\n",
            "  -5.62946750e-01 -3.27022551e-01]\n",
            " [-1.02289349e-01 -5.09142857e-01 -3.36671882e-01  9.13272691e-01\n",
            "   1.08681880e-03  7.54548772e-01  5.30814376e-01  4.91140854e-01\n",
            "   2.55042466e-01 -6.73542828e-01]\n",
            " [-2.87428618e-01 -8.35956781e-01 -1.77753089e-01  1.08155161e+00\n",
            "   2.61596882e-01  1.19329488e+00 -3.97691514e-01 -4.64830485e-01\n",
            "   8.08468285e-02  6.31726256e-01]\n",
            " [-3.01148415e-01 -2.43963098e-01  3.49425012e-01  9.63469423e-01\n",
            "   6.95798338e-02 -2.79081364e-01 -4.96983682e-01 -4.98002471e-01\n",
            "  -2.25728823e-02  1.01094238e+00]\n",
            " [ 5.05235345e-02 -1.11040338e+00  2.47624378e-01  4.71486122e-01\n",
            "   3.44079140e-01  2.84349627e-01  4.40935690e-01  1.79813393e-01\n",
            "  -9.93907826e-01  5.72640588e-01]\n",
            " [ 1.66267084e-01 -2.56262764e-01 -4.13071616e-01  1.95114412e-01\n",
            "  -9.67467786e-02 -1.16091162e+00 -6.74640464e-01  7.26084667e-02\n",
            "  -3.86524401e-01 -4.08351743e-01]\n",
            " [-4.41525251e-01  1.19743627e-01  2.91130350e-02  8.11073798e-01\n",
            "   1.39798081e-01  3.63133112e-01 -2.04744976e-01 -6.27047454e-01\n",
            "  -3.21221553e-01  7.59479971e-01]]\n",
            "shape (10, 10) [[ 0.12501378  0.17698607 -0.38993565  0.31189757  0.18000319 -0.00488049\n",
            "  -0.18442584 -0.19432288  0.0470514   0.42037141]\n",
            " [ 0.00906258 -0.18082814 -0.3600563   0.11874269  0.13692711 -0.39953254\n",
            "  -0.152837    0.37607959  0.36748241  0.4308325 ]\n",
            " [ 0.07980609 -0.65538197  0.21532471  0.21058692  0.49610478  0.22991021\n",
            "  -0.15882763  0.48566919 -0.10010413  0.27325211]\n",
            " [-1.19408415  0.15691424  0.06529382  0.19160219 -0.20880516 -0.82733587\n",
            "  -0.11317152  0.58220921 -0.08391827  0.35926667]\n",
            " [-0.51555832 -0.25522703 -0.37807641 -0.09092179 -0.52667423 -0.44577794\n",
            "  -0.35796637 -0.25994746  0.12190218 -0.35421446]\n",
            " [-0.56917501 -0.4306506  -0.04167637  0.42706296 -0.41982873  0.51440088\n",
            "  -0.02991488 -0.44004913  0.1070438  -0.31493185]\n",
            " [ 0.51082431  0.64807794  0.33426329  0.00281468 -0.03464719  0.01330417\n",
            "   0.34372804  0.52949827  0.57049814 -0.48931721]\n",
            " [ 1.1509627  -0.54552449 -0.76233466  0.33522953  0.17791122  0.99169998\n",
            "  -0.08174567 -0.03815328 -0.41708483  0.18452328]\n",
            " [-0.20484851 -0.20589527 -0.15002187 -0.27934144 -0.11443286 -0.17913149\n",
            "  -0.03918899 -0.066888   -0.47503797  0.43007643]\n",
            " [ 0.07861309 -0.20704573 -0.52386654  0.15305492  0.58199695  0.29877243\n",
            "  -0.19878349  0.38983714  0.26439421  0.43711863]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in bias_hidden_2_tanh:\n",
        "  print(\"shape\",i.shape,i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPHghBW-TbXq",
        "outputId": "65510b3f-dc27-463a-e542-e5b0ca03ffb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape (1, 10) [[ 1.05797892  0.31442825 -0.04317452  0.5029422   0.54482656  0.95860374\n",
            "   0.85314825  0.38274671  0.67048055  0.66138384]]\n",
            "shape (1, 10) [[-0.09431307 -0.40636118 -0.20781697  0.47979199 -0.01375025 -0.42810299\n",
            "  -0.36898373  0.16457903  0.00175803 -0.40470605]]\n",
            "shape (1, 10) [[-0.14738836 -0.10321176  0.07756606 -0.03278073  0.04718353 -0.10865822\n",
            "   0.08305507 -0.12282424  0.37758306 -0.02727477]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Relu Activation funtion "
      ],
      "metadata": {
        "id": "8k71x32PUUrK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weight_hidden_2_relu,bias_hidden_2_relu = neural_network(X,Y,[784,10,10,10],100,\"relu\",0.07,0,\"he\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dt7xp45rSuft",
        "outputId": "d70ef353-55b7-49b6-a96c-9dafc6998b43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs : 0\n",
            "Accuracy: 9.211666666666666 Cost:2.3900241360560646\n",
            "Epochs : 10\n",
            "Accuracy: 11.200000000000001 Cost:2.283593404475552\n",
            "Epochs : 20\n",
            "Accuracy: 12.653333333333332 Cost:2.2359520211441457\n",
            "Epochs : 30\n",
            "Accuracy: 14.045 Cost:2.191623841049514\n",
            "Epochs : 40\n",
            "Accuracy: 18.165 Cost:2.120736273643124\n",
            "Epochs : 50\n",
            "Accuracy: 25.856666666666666 Cost:2.004846117548963\n",
            "Epochs : 60\n",
            "Accuracy: 18.066666666666666 Cost:2.2593881604295354\n",
            "Epochs : 70\n",
            "Accuracy: 23.915 Cost:2.1724979489016336\n",
            "Epochs : 80\n",
            "Accuracy: 20.766666666666666 Cost:2.03342904915184\n",
            "Epochs : 90\n",
            "Accuracy: 23.696666666666665 Cost:1.9564053648431396\n",
            "training completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in weight_hidden_2_relu:\n",
        "  print(\"shape\",i.shape,i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uq6zYgKOTcEf",
        "outputId": "e1008313-dc1f-4889-ae2f-4be480383ec2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape (784, 10) [[ 0.11014994 -0.08798442  0.07074688 ... -0.00776997  0.04224745\n",
            "   0.09881605]\n",
            " [-0.02917801  0.0926358  -0.06326423 ...  0.05967225  0.05450347\n",
            "   0.0558639 ]\n",
            " [-0.03460228  0.02129302 -0.06436965 ... -0.04655502  0.0184495\n",
            "   0.06107269]\n",
            " ...\n",
            " [-0.05315242 -0.03499379  0.00555147 ... -0.06370007 -0.00568046\n",
            "   0.0300406 ]\n",
            " [-0.04049744  0.01755503 -0.10735557 ...  0.09232494  0.00036822\n",
            "   0.04159052]\n",
            " [-0.1088265   0.00064372  0.13488089 ... -0.06315799 -0.04385682\n",
            "   0.00940506]]\n",
            "shape (10, 10) [[ 0.74051483 -0.56431134  0.04533769 -0.41186784 -0.42471929 -0.0071292\n",
            "   0.03172553 -0.29514267 -0.20501289  0.14846359]\n",
            " [ 0.15953619 -0.28042935 -0.31373176  0.68238562  0.04343728 -0.57095574\n",
            "  -0.06350648 -0.0903493   0.48172023 -0.15723955]\n",
            " [-0.06794285 -0.04383926 -0.0641771  -0.59872548  0.36744074  1.03836389\n",
            "   0.24276419 -0.05938632 -0.21128829  0.59187557]\n",
            " [-0.13703374  0.78126465 -0.26537997  0.23531288 -0.66564231 -0.6244749\n",
            "   0.0421782   0.0167708   0.07946846 -0.47604575]\n",
            " [ 0.12075205 -0.44401316 -0.04551644  0.06164336 -0.46337877  0.40725345\n",
            "  -0.39072602  0.41456291 -0.21164139  0.22842422]\n",
            " [-0.91532894  0.83452273  0.14687385 -0.43604424 -0.14743309 -0.74556246\n",
            "  -0.16631685  0.92164211  0.0137259  -0.01844762]\n",
            " [ 0.57134025 -0.29226549 -0.82489624 -0.52713525 -0.31578724 -0.57904459\n",
            "   0.63779585  0.18099374  0.47010762  0.01069095]\n",
            " [-0.76998997 -0.70461458  0.21002642 -0.26892675 -0.40331421 -1.12234107\n",
            "  -0.85899207 -0.13568155 -0.15392478  0.31778692]\n",
            " [-0.41542012 -0.13337306 -0.29298639 -0.29184838 -0.42946912 -0.30935896\n",
            "  -0.06201979 -0.36406006  0.32085937 -0.22185717]\n",
            " [ 0.855037    0.9201268  -0.27797033 -0.05111062  0.09173704 -0.35358634\n",
            "  -0.45539103 -0.88420473  0.52747692  0.62057241]]\n",
            "shape (10, 10) [[ 0.91831209  0.14129733 -0.00290866  0.47274675  0.59364762  0.53057036\n",
            "   0.18837513 -0.25830022 -0.07270148 -0.51728664]\n",
            " [ 0.08323615  0.02553421  0.3406669  -0.37454872  0.13772042 -0.05501236\n",
            "  -0.15605185  0.12638504 -0.01738488  0.6416863 ]\n",
            " [ 0.36932127  0.15498929 -0.79389603  0.4720868  -0.24862686  0.00830364\n",
            "   0.06607852  0.3329805   0.10792885  0.08931577]\n",
            " [-0.18569374 -0.72219066  0.10130789 -0.24647941  0.19528286 -0.29951575\n",
            "  -0.65668456  0.11513343 -0.2957682   0.01599138]\n",
            " [ 0.42440228  0.02237236 -0.92196834  0.34942879 -0.68594953  0.54847719\n",
            "   0.21681571 -0.400413    0.69878856  0.02418094]\n",
            " [-0.29521267  0.2394225   0.28852595 -0.00634387 -0.41373646 -0.05903478\n",
            "   0.25385503 -0.50550204  0.22974035 -0.36905552]\n",
            " [-0.30277478 -0.81278709 -0.5547284   0.14565233 -0.26211341 -0.09149669\n",
            "  -0.67897853 -0.57421688  0.09918151 -0.13656773]\n",
            " [-0.60382722 -0.34208106  0.79010608 -0.17398402  0.47338378 -0.66439156\n",
            "   0.76557933 -0.58787364  0.02799945  0.17880835]\n",
            " [ 0.33050276  0.28280414  0.65903915 -0.40579255 -0.45829804  0.10862647\n",
            "  -0.19052282 -0.45146191 -0.40468059 -0.14874255]\n",
            " [-0.5055591  -0.05051406  0.19633915 -0.4285087  -0.67611086 -0.23924151\n",
            "  -0.08705437 -0.3439445  -0.47232935 -0.38693512]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in bias_hidden_2_relu:\n",
        "  print(\"shape\",i.shape,i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5y7hXNwkTb4v",
        "outputId": "b4fe0b9c-7820-4b69-cacd-4b539452ab80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape (1, 10) [[ 2.43874310e-02  3.32056129e-04 -2.51431905e-01  1.26702317e-03\n",
            "   8.63554917e-02  1.02591591e-02 -7.00512092e-02  2.40831291e-03\n",
            "  -3.44567712e-01  6.34948147e-03]]\n",
            "shape (1, 10) [[-0.16537831  0.00765883  0.00796901  0.01651085 -0.00342235 -0.06923109\n",
            "  -0.1775626  -0.01839671 -0.07396728 -0.08916667]]\n",
            "shape (1, 10) [[-0.11689366  0.17307672 -0.37140419 -0.07983347 -0.00309773 -0.06206476\n",
            "  -0.06523411  0.31066559  0.06486062  0.20414297]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 6"
      ],
      "metadata": {
        "id": "ogGd6FiS8Vw7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extend your code from problem 5 to implement momentum with your gradient descent. The  momentum value will be passed as a parameter. Your function should perform “epoch” number of  epochs and return the resulting weights."
      ],
      "metadata": {
        "id": "VfwFvzth8Y7Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "momentum = 0.9\n",
        "weight_hidden_2_momentum,bias_hidden_2_momentum = neural_network(X,Y,[784,10,10,10],100,\"tanh\",0.2,momentum,\"he\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SsO3Mjc28bgX",
        "outputId": "4f48db82-d3d0-435b-d261-9e25544786ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs : 0\n",
            "Accuracy: 10.81 Cost:2.4246472234479715\n",
            "Epochs : 10\n",
            "Accuracy: 13.306666666666667 Cost:2.4927126755629065\n",
            "Epochs : 20\n",
            "Accuracy: 14.278333333333334 Cost:2.288821166113774\n",
            "Epochs : 30\n",
            "Accuracy: 15.976666666666667 Cost:2.3576963981269734\n",
            "Epochs : 40\n",
            "Accuracy: 12.833333333333332 Cost:2.3448994290027936\n",
            "Epochs : 50\n",
            "Accuracy: 5.65 Cost:2.377629138795824\n",
            "Epochs : 60\n",
            "Accuracy: 6.7683333333333335 Cost:2.4845010720505387\n",
            "Epochs : 70\n",
            "Accuracy: 15.501666666666667 Cost:2.3034441653071953\n",
            "Epochs : 80\n",
            "Accuracy: 21.758333333333333 Cost:2.188605570292917\n",
            "Epochs : 90\n",
            "Accuracy: 22.356666666666666 Cost:2.204896953666272\n",
            "training completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in weight_hidden_2_momentum:\n",
        "  print(\"shape\",i.shape,i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7jZaZrJUNmO",
        "outputId": "2c1b3226-c0f3-441e-eb68-e2ebd3ae04e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape (784, 10) [[ 0.07434923 -0.00397005 -0.05620124 ... -0.01584039  0.03258238\n",
            "  -0.00689419]\n",
            " [-0.0277622   0.00218493 -0.08066819 ...  0.04090915 -0.02733304\n",
            "   0.01310768]\n",
            " [ 0.0025471   0.11238279 -0.04423048 ...  0.04401599  0.03775988\n",
            "  -0.04514117]\n",
            " ...\n",
            " [ 0.11361575  0.07326121 -0.00461883 ... -0.00972068 -0.04098391\n",
            "   0.0657481 ]\n",
            " [-0.01793086  0.05181419  0.01345883 ...  0.0009351   0.02380599\n",
            "  -0.04219216]\n",
            " [ 0.03310634  0.01766649  0.01995829 ...  0.00435305  0.08472738\n",
            "  -0.11835821]]\n",
            "shape (10, 10) [[-1.88133831  0.52613907 -0.85058873 -0.65210226 -0.31525091  0.27969625\n",
            "   0.63221791  0.66507093  1.56658925 -0.79434336]\n",
            " [ 0.66702685 -0.13597491  0.31891837 -0.28267301 -0.44971518  0.86355978\n",
            "  -0.38455559  0.28133585  0.10778709 -0.2053669 ]\n",
            " [ 0.49916463  0.42290074  0.05428675 -0.41172472 -0.4219439  -0.60028569\n",
            "  -0.00796702  0.15058193 -0.37507776 -0.25988558]\n",
            " [ 0.26527469 -0.02717921  0.03447945 -0.4028924   0.56623128  0.33107743\n",
            "  -0.03129915  0.13523664  0.12581879  0.09417008]\n",
            " [ 0.24351381  0.49862423  0.79359321 -0.5638167  -0.76728604  0.40776338\n",
            "   0.16150282  0.29243401 -0.71476765  0.46476657]\n",
            " [ 0.28388486  0.78092928  0.09818212  0.0304527  -0.10169129 -0.30735693\n",
            "  -0.6948999   0.59834773 -0.750215   -0.14834175]\n",
            " [ 0.38733042  0.05279222  0.6265779  -0.23016348  0.61623648  0.10273704\n",
            "   0.28871398 -0.30653184 -0.42577509  0.21860641]\n",
            " [ 1.30036444 -1.46663086  0.14324632  1.00180634  0.54205207  0.06253596\n",
            "  -0.20137733  0.58324791  0.1550622   0.26693458]\n",
            " [-1.17913672 -0.42320237 -0.06932064 -0.48584426 -0.11844526 -0.17586734\n",
            "  -0.35085157 -0.23306996 -0.2817117  -1.14031206]\n",
            " [-0.67620525 -0.15416602 -0.20484881  0.31840125  0.04641187  0.45821418\n",
            "   0.60705733 -0.05446908  0.45461666 -0.46266055]]\n",
            "shape (10, 10) [[ 0.21314748  0.46222049 -0.32950293 -0.98932914  0.09250219  0.2437383\n",
            "   0.08944225 -0.4453604   0.50649426  0.14248394]\n",
            " [ 0.28774474 -0.36634691  0.66943485  0.00114809 -0.33540447  0.54977374\n",
            "  -0.31018382  0.18295227 -0.61566231  0.80927636]\n",
            " [-0.49390621  0.3833626   0.57744012  0.5478567  -0.26036889  0.40808442\n",
            "   0.24911373 -0.44783344 -0.03626105 -0.29561508]\n",
            " [ 1.14502031 -0.77512306  0.09593148 -0.22343351 -0.21863864 -0.09798516\n",
            "   0.18343388  0.12726166 -0.11709714  0.19839217]\n",
            " [ 0.11616369 -0.61818691  0.21998663 -0.22769457  0.20552439  0.1471096\n",
            "  -0.54447845  0.31566557 -0.53078092  0.49690908]\n",
            " [-0.31747702 -0.61608001  0.05628995 -0.00918273  0.42109437 -0.0110777\n",
            "   0.25411746  0.38926721 -0.01498532  0.39650636]\n",
            " [-0.2831884   0.71463407 -0.29899038 -0.48047765 -0.09510398  0.63211172\n",
            "   0.86484497 -0.59774444  0.31907565  0.29654142]\n",
            " [ 0.06059087  0.0959648  -0.39910855 -0.29026744 -0.22382348 -0.18338074\n",
            "   0.25172244 -0.41269289 -0.51691197  0.19993595]\n",
            " [-0.88089078  0.4404204   0.09977936  0.11985781 -0.49069884  0.48907179\n",
            "   0.09962332 -0.78899883  0.01136603  0.23212138]\n",
            " [-0.05633653 -0.49396134 -0.76705051  0.10235859  0.41584085  0.05028729\n",
            "   0.53496152 -0.24600365 -0.07498579  0.11979678]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in bias_hidden_2_momentum:\n",
        "  print(\"shape\",i.shape,i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tDQ7y1YUO7m",
        "outputId": "04262227-02fc-4835-a2a8-4602180b3b57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape (1, 10) [[ 2.69555846 -0.36332662 -0.20858925 -0.2983627  -0.24838776  0.04400455\n",
            "  -0.31607882  1.36211371  0.5939772   0.52202047]]\n",
            "shape (1, 10) [[-0.64371937 -0.29753769 -0.29608504  0.14639424  0.10229798  0.22196268\n",
            "  -0.26687856 -0.34373464  0.20185435 -0.44806351]]\n",
            "shape (1, 10) [[-0.19101454  0.35858567 -0.10727037  0.17603227  0.31504488  0.10945887\n",
            "   0.03300546 -0.29126452 -0.16604493 -0.16986692]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# d"
      ],
      "metadata": {
        "id": "aKuH_H4lnRJs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP:\n",
        "\n",
        "  def __init__(self,X,Y,layer_nodes,activation_type):\n",
        "    self.X = X\n",
        "    self.Y = Y\n",
        "    self.layer_nodes = layer_nodes\n",
        "    self.activation_type = activation_type \n",
        "\n",
        "  def loss(self,y_actual,y_pred):\n",
        "    '''\n",
        "    Calculating cost \n",
        "    '''\n",
        "    cost = -np.sum(y_actual*np.log(y_pred))          #calculating the cost      categorical cross entropy \n",
        "    return cost/y_actual.shape[0]\n",
        "\n",
        "  # #softmax function \n",
        "\n",
        "  def softmax(self,x, derivative=False):\n",
        "    exp_x = np.exp(x)  \n",
        "    softmax = exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "    if derivative:\n",
        "        return softmax * (1 - softmax)\n",
        "    return softmax\n",
        "\n",
        "\n",
        "  def one_hot_encoded(self,x):\n",
        "    '''\n",
        "    one hot encoding \n",
        "    '''\n",
        "    n_values = 10     #number of categories (for mnist categories = 10)\n",
        "    y=np.eye(n_values)[x]     \n",
        "    return y\n",
        "\n",
        "  def relu(self,x,derivative=False):\n",
        "    '''\n",
        "    Relu Activation\n",
        "    '''\n",
        "    if derivative:\n",
        "        return np.where(x <= 0, 0, 1)\n",
        "    else:\n",
        "        return np.maximum(0, x)\n",
        "  \n",
        "  #tanh function \n",
        "  def tanh(self,x,derivative=False):\n",
        "    '''\n",
        "    tanh Activation \n",
        "    '''\n",
        "    t=np.tanh(x)\n",
        "    if derivative:\n",
        "         return (1-t**2)\n",
        "    return t\n",
        "  \n",
        "  def sigmoid(self,x,derivative =False):\n",
        "    sig = 1 / (1 + np.exp(-x))\n",
        "    if derivative:\n",
        "        return sig * (1 - sig)\n",
        "    return sig\n",
        "\n",
        "\n",
        "  def Activation(self,input,function_type,derivative=False):\n",
        "    '''\n",
        "    Activation call\n",
        "    '''\n",
        "    if function_type == \"sigmoid\":\n",
        "      if derivative:\n",
        "        self.sigmoid(input,True)\n",
        "      return self.sigmoid(input,False)\n",
        "\n",
        "    if function_type == \"relu\":\n",
        "      if derivative:\n",
        "        self.relu(input,True)\n",
        "      return self.relu(input,False)\n",
        "\n",
        "    if function_type == \"tanh\":\n",
        "      if derivative:\n",
        "        self.tanh(input,True)\n",
        "      return self.tanh(input,False)\n",
        "\n",
        "\n",
        "  def train(self,epochs,learning_rate,momentum,initialization):\n",
        "     \n",
        "     weights = [None]*(len(self.layer_nodes)-1)\n",
        "     bias = [None]*(len(self.layer_nodes)-1)\n",
        "     Z = [None]*(len(self.layer_nodes)-1)\n",
        "     A = [None]*(len(self.layer_nodes)-1)\n",
        "     Vdw = [None]*(len(self.layer_nodes)-1)\n",
        "     Vdb = [None]*(len(self.layer_nodes)-1)\n",
        "     \n",
        "     for i in range(len(self.layer_nodes)-1):\n",
        "         if initialization == \"xavier\":\n",
        "            # Xavier initialization \n",
        "            weights[i] = np.random.randn(self.layer_nodes[i], self.layer_nodes[i+1]) * np.sqrt(1/self.layer_nodes[i])\n",
        "         elif initialization == \"he\":\n",
        "            # He initialization \n",
        "            weights[i] = np.random.randn(self.layer_nodes[i], self.layer_nodes[i+1]) * np.sqrt(2/self.layer_nodes[i])\n",
        "         bias[i] = np.random.rand(1,self.layer_nodes[i+1])\n",
        "         Vdw[i] = np.zeros_like(weights[i]) # initializing the momentum term\n",
        "         Vdb[i] = np.zeros_like(bias[i]) # initializing the momentum term\n",
        "\n",
        "     y = one_hot_encoded(self.Y)\n",
        "     x = self.X/255.0\n",
        "     for i in range(epochs):\n",
        "          '''forward pass '''\n",
        "          inputs = x\n",
        "          for j in range(len(self.layer_nodes)-2):\n",
        "             Z[j]  = np.add(np.matmul(inputs,weights[j]),bias[j])\n",
        "             A[j] = self.Activation(Z[j],self.activation_type,False)\n",
        "             inputs =  A[j]\n",
        "          Z[-1] = np.add(np.matmul(inputs,weights[-1]),bias[-1])\n",
        "          y_pred = A[-1] = self.softmax(Z[-1],False)\n",
        "\n",
        "          cost =  self.loss(y,y_pred)     #calculating cost \n",
        "          '''backward pass '''\n",
        "          delta = [None] * (len(self.layer_nodes) - 1)\n",
        "          delta[-1] = y_pred - y\n",
        "          # print(\"delta -1\",delta[-1])\n",
        "          for j in range(len(self.layer_nodes) - 2, 0, -1):\n",
        "              delta[j-1] = (np.matmul(delta[j], weights[j].T) * Activation(Z[j-1], self.activation_type, True))\n",
        "\n",
        "          # weight and bias updates\n",
        "          for j in reversed(range(len(self.layer_nodes)-1)):\n",
        "\n",
        "            dW = np.matmul(A[j-1].T, delta[j])/y.shape[0]  if j != 0 else np.matmul(x.T, delta[j])/y.shape[0] \n",
        "            db = np.sum(delta[j], axis=0, keepdims=True)/y.shape[0] \n",
        "\n",
        "            Vdw[j] = momentum*Vdw[j] + (1-momentum)*dW\n",
        "            Vdb[j] = momentum*Vdb[j] + (1-momentum)*db\n",
        "\n",
        "            weights[j] = weights[j] - learning_rate * Vdw[j]\n",
        "            bias[j] = bias[j] - learning_rate * Vdb[j]\n",
        "\n",
        "\n",
        "          if i%10 == 0:\n",
        "             print(\"Epochs :\",i)\n",
        "             y_temp = np.argmax(y_pred, axis = 1)\n",
        "             accuracy = (Y == y_temp).sum() / len(Y)\n",
        "             print(f\"Accuracy: {accuracy*100} Cost:{cost}\")\n",
        "          elif i == epochs-1:\n",
        "             print('training completed')\n",
        "\n",
        "     return weights,bias "
      ],
      "metadata": {
        "id": "KVqT0F-5m2uV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_mlp = MLP(X,Y,[784,10,10,10],\"sigmoid\")\n",
        "weights,bias=model_mlp.train(100,0.07,0.9,\"he\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1zMDG37K1Px",
        "outputId": "d616e65d-9379-4467-d98f-5e41f9d007c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs : 0\n",
            "Accuracy: 9.871666666666666 Cost:2.4424202280116467\n",
            "Epochs : 10\n",
            "Accuracy: 9.871666666666666 Cost:2.3580660601014842\n",
            "Epochs : 20\n",
            "Accuracy: 11.553333333333333 Cost:2.2871553369285187\n",
            "Epochs : 30\n",
            "Accuracy: 19.121666666666666 Cost:2.246555429933913\n",
            "Epochs : 40\n",
            "Accuracy: 25.948333333333334 Cost:2.2204032389317767\n",
            "Epochs : 50\n",
            "Accuracy: 30.196666666666665 Cost:2.208334139943309\n",
            "Epochs : 60\n",
            "Accuracy: 28.878333333333334 Cost:2.2061389494614705\n",
            "Epochs : 70\n",
            "Accuracy: 29.301666666666666 Cost:2.206511210298669\n",
            "Epochs : 80\n",
            "Accuracy: 29.883333333333333 Cost:2.2058793975704427\n",
            "Epochs : 90\n",
            "Accuracy: 29.645 Cost:2.2044861344471376\n",
            "training completed\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}